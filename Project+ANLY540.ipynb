{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webhoseio, json\n",
    "import urllib, sys, os, requests\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "\n",
    "ArticleListFile = 'f:\\\\anly540.json'\n",
    "url= \"f:\\\\TextToProcess.txt\"\n",
    "\n",
    "print('Starting Program')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "webhoseio.config(token=\"a7e5472d-d08b-478e-915a-d036a99f6712\")\n",
    "\n",
    "query_params = { \"q\": \"organization:Tesla\", \"ts\": \"1509161113588\", \"sort\": \"crawled\" }\n",
    "\n",
    "print('Querying data')\n",
    "output = webhoseio.query(\"filterWebContent\", query_params)\n",
    "\n",
    "print('Data quieries: ')\n",
    "\n",
    "print(len(output['posts']))\n",
    "\n",
    "    \n",
    "#Get the next batch of posts\n",
    "x=0\n",
    "#Each query is giving 100 feeds. for 900, the loop runs 9 times.\n",
    "while len(output['posts'])<1000:\n",
    "    output1 = webhoseio.get_next()\n",
    "    x=x+1\n",
    "    print(str(x) + ': '+ str(len(output1['posts'])))\n",
    "    for itm in output1['posts']:\n",
    "        output['posts'].append(itm)\n",
    "\n",
    "print('loop complete')\n",
    "\n",
    "outfile = open(ArticleListFile, 'w')\n",
    "\n",
    "print('Saving file')\n",
    "\n",
    "json.dump(output, outfile)\n",
    "\n",
    "print('file saved')\n",
    "\n",
    "outfile.close()\n",
    "\n",
    "print('End program')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reading file')\n",
    "outfileRead=open(ArticleListFile, 'r')\n",
    "\n",
    "print('Getting data in dictionary')\n",
    "data=json.load(outfileRead)\n",
    "\n",
    "TextToProcess=''\n",
    "\n",
    "for y in range(0, 999):\n",
    "    TextToProcess = TextToProcess + data['posts'][y]['title'] + '\\n'\n",
    "\n",
    "\n",
    "f = open(url,'wb')\n",
    "f.write(str.encode(TextToProcess))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= open(url,'rb')\n",
    "\n",
    "    \n",
    "#Calais API key.\n",
    "x_ag_access_token1 = 'bq9XnoneGq87PB5yxjjDZdG3GpmuxfUC' \n",
    "\n",
    "#REST URL of CAlais.\n",
    "calaisREST_URL = 'https://api.thomsonreuters.com/permid/calais' \n",
    "\n",
    "##### send data to Calais API.\n",
    "header = {'X-AG-Access-Token' : x_ag_access_token1, 'Content-Type' : 'text/plain', 'outputformat' : 'application/json'}\n",
    "\n",
    "print(\"Sending\")\n",
    "##### get API results and print them.\n",
    "results =  requests.post(calaisREST_URL, data=file, headers=header)\n",
    "print (results)\n",
    "print (results.text)\n",
    "\n",
    "output_file_name = os.path.basename('Entities') + '.xml'\n",
    "output_file = open(os.path.join('f:\\\\', output_file_name), 'wb')\n",
    "output_file.write(results.text.encode('utf-8'))\n",
    "output_file.close()\n",
    "\n",
    "file.close()\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk, re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def tokenize(text):\n",
    "    # first tokenize by sentence, then by word to ensure that\n",
    "    # punctuation is caught as it's own token\n",
    "\n",
    "    tokens  = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word not in stopwords]\n",
    "    lmtzr   = WordNetLemmatizer()\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token.lower())\n",
    "            \n",
    "    bigrams  = [' '.join(bigram) for bigram in nltk.bigrams(filtered_tokens)]\n",
    "    trigrams = [' '.join(trigram) for trigram in nltk.trigrams(filtered_tokens)]\n",
    "    stems = [lmtzr.lemmatize(t,'v') for t in filtered_tokens]\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        if bigram not in stopwords:\n",
    "            stems.append(bigram)\n",
    "            \n",
    "    for trigram in trigrams:\n",
    "        if trigram not in stopwords:\n",
    "            stems.append(trigram)\n",
    "            \n",
    "    return stems\n",
    "\n",
    "\n",
    "\n",
    "def clstr_lda(num_topics, stories):\n",
    "    \"\"\"\n",
    "    \n",
    "    :rtype: object\n",
    "    \"\"\"\n",
    "    \n",
    "    n_top_words = 10\n",
    "    \n",
    "    tf_vectorizer = CountVectorizer(max_df=0.8, min_df=0.2, max_features=1000,tokenizer=tokenize, ngram_range=(1,3))\n",
    "    tf = tf_vectorizer.fit_transform(stories)\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, max_iter=200, learning_method='online', learning_offset=10.,random_state=1)\n",
    "    lda.fit(tf)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    #print(\"\\nLDA Topics:\")\n",
    "    # print top topic words\n",
    "    \n",
    "    topics = dict()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        topics[topic_idx] = [tf_feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" | \".join([tf_feature_names[i]\n",
    "                          for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        #print()\n",
    "    return topics\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    myinput = []\n",
    "    \n",
    "    print('Reading file')\n",
    "    outfileRead=open('F:\\\\anly540.json', 'r')\n",
    "    \n",
    "    print('Getting data in dictionary')\n",
    "    \n",
    "    data=json.load(outfileRead)\n",
    "    \n",
    "    for y in range(0, 999):\n",
    "        myinput.append(data['posts'][y]['text'])\n",
    "\n",
    "\n",
    "topics = clstr_lda(100, myinput)\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('f:\\\\topics.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in topics.items():\n",
    "       writer.writerow([key, value])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, operator\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path = 'C:/Users/Rahul/OneDrive/Harrisburg/ANLY 540/Assignment 6/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model...\n",
      "Finished loading Word2Vec model...\n"
     ]
    }
   ],
   "source": [
    "def load_wordvec_model(modelName, modelFile, flagBin, limit):\n",
    "    print('Loading ' + modelName + ' model...')\n",
    "    \n",
    "    if limit > 0:\n",
    "        model = KeyedVectors.load_word2vec_format(model_path + modelFile, binary=flagBin, limit=limit)\n",
    "    else:\n",
    "        model = KeyedVectors.load_word2vec_format(model_path + modelFile, binary=flagBin)\n",
    "    \n",
    "    print('Finished loading ' + modelName + ' model...')\n",
    "    return model\n",
    "\n",
    "model_word2vec = load_wordvec_model('Word2Vec', 'GoogleNews-vectors-negative300.bin.gz', True, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# develop own topic taxonomy\n",
    "topic_taxonomy = {\n",
    "        \"time chronology measurement\":\n",
    "        {\n",
    "            \"chonology\": \"today back future continue ago\",\n",
    "            \"day component\": \"october wednesday year september november\", \n",
    "            \"time scale\": \"delay period quarter months week\"\n",
    "        },\n",
    "        \"product making\":\n",
    "        {\n",
    "            \"product manufacturing\": \"production produce build manufacture\", \n",
    "            \"product categorization\": \"make model\", \n",
    "            \"creation\": \"new become increase\",\n",
    "            \"introduction\": \"release launch\"\n",
    "        },\n",
    "        \"numeric sequence\":\n",
    "        {\n",
    "            \"number\": \"one two three\",\n",
    "            \"scales\": \"million billion\", \n",
    "            \"sequential\": \"second first last next recent current\"\n",
    "        },\n",
    "        \"object transfer\":\n",
    "        {\n",
    "            \"object handover\": \"give provide sell push\", \n",
    "            \"object arrangement\": \"set trade sales offer\", \n",
    "            \"Object takeover\": \"get take use buy earn\"\n",
    "        },\n",
    "        \"Business company\":\n",
    "        {\n",
    "            \"Business name\": \"tsla tesla\", \n",
    "            \"Business type\": \"company firm industry business\",\n",
    "            \"Business stakeholder\": \"CEO lead invester people world analysts\", \n",
    "            \"Business stakes\": \"market share investment stock loss hold\"\n",
    "        },\n",
    "        \"Technology object manipulation\":\n",
    "        {\n",
    "            \"Device feature\": \"technology system\", \n",
    "            \"components and energy\": \"electric battery power charge energy\", \n",
    "            \"manipulation\": \"go start operate work move\"\n",
    "        }\n",
    "} \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec_similarity(input1, input2, vectors):\n",
    "    term_vectors = [np.zeros(300), np.zeros(300)]\n",
    "    terms = [input1, input2]\n",
    "        \n",
    "    for index, term in enumerate(terms):\n",
    "        for i, t in enumerate(term.split(' ')):\n",
    "            try:\n",
    "                term_vectors[index] += vectors[t]\n",
    "            except:\n",
    "                term_vectors[index] += 0\n",
    "        \n",
    "    result = (1 - spatial.distance.cosine(term_vectors[0], term_vectors[1]))\n",
    "    if result is 'nan':\n",
    "        result = 0\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function checks whether the input words are present in the vocabulary for the model\n",
    "def vocab_check(vectors, words):\n",
    "    \n",
    "    output = list()\n",
    "    for word in words:\n",
    "        if word in vectors.vocab:\n",
    "            output.append(word.strip())\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function calculates similarity between two strings using a particular word vector model\n",
    "def calc_similarity(input1, input2, vectors):\n",
    "    s1words = set(vocab_check(vectors, input1.split()))\n",
    "    s2words = set(vocab_check(vectors, input2.split()))\n",
    "    \n",
    "    output = vectors.n_similarity(s1words, s2words)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function takes an input string, runs similarity for each item in topic_taxonomy, sorts and returns top 3 results\n",
    "def classify_topics(input, vectors):\n",
    "    feed_score = dict()\n",
    "    for key, value in topic_taxonomy.items():\n",
    "        max_value_score = dict()\n",
    "        for label, keywords in value.items():\n",
    "            max_value_score[label] = 0\n",
    "            topic = (key + ' ' + keywords).strip()\n",
    "            max_value_score[label] += float(calc_similarity(input, topic, vectors))\n",
    "            \n",
    "        sorted_max_score = sorted(max_value_score.items(), key=operator.itemgetter(1), reverse=True)[0]\n",
    "        feed_score[sorted_max_score[0]] = sorted_max_score[1]\n",
    "    return sorted(feed_score.items(), key=operator.itemgetter(1), reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Business stakeholder', 0.5232430912801942), ('product categorization', 0.3393615919584506), ('components and energy', 0.30000098325734914)]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # example using Word2Vec\n",
    "    output1 = classify_topics('Tesla Company CEO Elon Musk is considered visionary and young entrepreneur idol', model_word2vec)\n",
    "    print(output1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
